容器网络
容器可以指定使用宿主机的网络栈，但在大多数情况下，我们都希望容器进程能使用自己 Network Namespace 里的网络栈，即：拥有属于自己的 IP 地址和端口；
这就引发了一个问题，这个被隔离的容器进程，该如何跟其他 NetworkNamespace 里的容器进程进行交互呢？
答：主要通过 docker0网桥（相当于交换机）+Veth Pair（一对设备，相当于网线的两头）
  docker容器内的路由规则会将数据交给Veth Pair从而流向docker0网桥，网桥相当于交换机的作用，会将数据（包括arp广播）交给对应端口，从而流向目标容器
  
在宿主机上访问本机上的容器，数据的走向是如何的？
答：根据宿主机上的路由规则，访问本机容器ip的数据都会交给docker0网桥，从而到达容器内部

容器访问其他宿主机的数据走向？
答：当数据到达docker0网桥后，根据路由规则会将数据交给宿主机网卡，地址转换之后流向目标宿主机（这里要求开启宿主机的路由转发功能：echo "1" > /proc/sys/net/ipv4/ip_forward）

从Flannel 项目来说“跨主机通信”
Flannel 项目本身只是一个框架，真正为我们提供容器网络功能的，是 Flannel 的后端实现。目前，Flannel 支持三种后端实现，分别是：
  1. VXLAN；
  2. host-gw；
  3. UDP

先来介绍下TUN 设备（Tunnel 设备）：一种工作在三层（Network Layer）的虚拟网络设备，TUN 设备的功能非常简单，即 在操作系统内核和用户应用程序之间传递 IP 包
1）UDP模式
  是 Flannel 项目最早支持的一种方式，却也是性能最差的一种方式。所以这个模式目前已经被弃用，不过这种模式是最直接、也是最容易理解的容器跨主网络实现，所以最先介绍这种模式
  当 IP 包从容器经过 docker0 出现在宿主机，然后又根据路由表进入 flannel0 设备后，宿主机上的 flanneld 进程（Flannel 项目在每个宿主机上的主进程），就会收到这个 IP包；
  然后，flanneld 看到了这个 IP 包的目的地址，就把它发送给了 目标容器的宿主机；
    这里就有个问题，flanneld进程是如何知道容器跟宿主机的对应关系呢？
    答：利用子网，该模式下每台宿主机都会分配一个子网，其上的容器使用的都是这个子网内的网络，而这些子网与宿主机的对应关系，正是保存在 Etcd 当中
  所以说，flanneld 在收到 container-1 发给 container-2 的 IP 包之后，就会把这个 IP 包直接封装在一个 UDP 包里（每台宿主机上的 flanneld，都监听着一个 8285 端口），然后发送给 Node 2；
  不难理解，这个 UDP 包的源地址，就是flanneld 所在的 Node 1 的地址，而目的地址，则是 container-2 所在的宿主机 Node 2 的地址
    为什么使用UDP协议呢？不怕丢包吗
    答：偶尔丢了也无所谓，可以把这个阶段看作网路中的某一环，网络本来就是不可靠的，如果是两台宿主机上的容器进行通信，假设使用TCP，那么他们的“可靠”是由这两端来保证的
  通过这样一个普通的、宿主机之间的 UDP 通信，一个 UDP 包就从 Node 1 到达了 Node2。而 Node 2 上监听 8285 端口的进程也是 flanneld，所以这时候，flanneld 就可以从这个UDP 包里解析出封装在里面的、container-1 发来的原 IP 包；
  而接下来 flanneld 的工作就非常简单了：flanneld 会直接把这个 IP 包发送给它所管理的TUN 设备，即 flannel0 设备
  需要注意的是，上述流程要正确工作还有一个重要的前提，那就是 docker0 网桥的地址范围必须是 Flannel 为宿主机分配的子网，这个可以在启动容器时指定参数来实现
  总结下整个流程：
  container1（NODE1） --路由--> docker0（NODE1） --路由--> flannel0（NODE1） --从内核态到达用户态--> flanneld（NODE1） --添加UDP头部--> eth0（NODE1）--网络发送--> 
    eth0（NODE2）--根据UDP端口交给flanneld进程处理--> flanneld（NODE2） --从用户态到达内核态--> flannel0（NODE2）--路由--> docker0（NODE2）--路由--> container2（NODE2）
  这种模式相当于给两个容器之间开辟来一条隧道，但是存在严重的性能问题，因为数据包要交给flanneld进程（用户态）处理，所以仅在发出 IP 包的过程中，就需要经过三次用户态与内核态之间的数据拷贝：
    第一次：用户态的容器进程发出的 IP 包经过 docker0 网桥进入内核态；
    第二次：IP 包根据路由表进入 TUN（flannel0）设备，从而回到用户态的 flanneld 进程；
    第三次：flanneld 进行 UDP 封包之后重新进入内核态，将 UDP 包通过宿主机的 eth0 发出去
  所以说，我们在进行系统级编程的时候，有一个非常重要的优化原则，就是要减少用户态到内核态的切换次数，并且把核心的处理逻辑都放在内核态进行
2）VXLAN模式
VXLAN是 Linux 内核本身就支持的一种网络虚似化技术，所以说VXLAN 可以完全在内核态实现上述封装和解封装的工作，从而通过与前面相似的“隧道”机制，构建出覆盖网络（Overlay Network）
其设计思想是：在现有的三层网络之上，“覆盖”一层虚拟的、由内核VXLAN 模块负责维护的二层网络，使得连接在这个 VXLAN 二层网络上的“主机”（虚拟机或者容器都可以）之间，可以像在同一个局域网（LAN）里那样自由通信
而为了能够在二层网络上打通“隧道”，VXLAN 会在宿主机上设置一个特殊的网络设备作为“隧道”的两端，这个设备就叫作 VTEP，即：VXLAN Tunnel End Point（虚拟隧道端点），其作用跟前面的 flanneld 进程非常相似，只不过它进行封装和解封装的对象，是二层数据帧（Ethernet frame）



